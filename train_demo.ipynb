{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: FlashAttention Not Installed, when initializing model make sure to use default Transformers\n"
     ]
    }
   ],
   "source": [
    "## loading in libraries\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import hydra\n",
    "import pandas as pd\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import SchedulerType, get_scheduler\n",
    "\n",
    "## initialize the model\n",
    "from Heimdall.models import Heimdall_Transformer, TransformerConfig\n",
    "\n",
    "## Cell representation tools from heimdall\n",
    "from Heimdall.cell_representations import Cell_Representation\n",
    "from Heimdall.f_g import identity_fg\n",
    "from Heimdall.f_c import geneformer_fc\n",
    "from Heimdall.utils import heimdall_collate_fn\n",
    "from Heimdall.trainer import Heimdall_Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cell Representation Object\n",
    "\n",
    "- Here you define the f_g and the f_c that you want to use. Here we use ones pre-made and stored in the \n",
    "files `Heimdall.f_g` and `Heimdall.f_c`. \n",
    "\n",
    "- Follow the readme and the notion page for how to design f_g and f_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nzh/miniconda3/envs/heimdall/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Finished Loading in data/sc_sub_nick.h5ad\n",
      "> Normalizing anndata...\n",
      "> Log Transforming anndata...\n",
      "> Using highly variable subset... top 1000 genes\n",
      "> Scaling the data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nzh/miniconda3/envs/heimdall/lib/python3.10/site-packages/scanpy/preprocessing/_scale.py:299: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Finished Processing Anndata Object\n",
      "> Performing the f_g identity, desc: each gene is its own token\n",
      "> Finished calculating f_g with identity\n",
      "> Performing the f_c using rank-based values, as seen in geneformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26553/26553 [00:06<00:00, 4321.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Finished calculating f_c with identity\n",
      "> Finished extracting labels, self.labels.shape: (26553,)\n",
      "Cell representation X: (26553, 1000)\n",
      "Cell labels y: (26553,)\n"
     ]
    }
   ],
   "source": [
    "with hydra.initialize(version_base=None, config_path=\"config\"):\n",
    "    config = hydra.compose(config_name=\"config\") ## setting up a default experiment\n",
    "    # print(OmegaConf.to_yaml(config))\n",
    "\n",
    "CR = Cell_Representation(config) ## takes in the whole config from hydra\n",
    "CR.preprocess_anndata() ## standard sc preprocessing can be done here\n",
    "CR.preprocess_f_g(identity_fg) ## takes in the identity f_g specified above\n",
    "CR.preprocess_f_c(geneformer_fc) ## takes in the geneformer f_c specified above\n",
    "CR.prepare_labels() ## prepares the labels\n",
    "\n",
    "## we can take this out here now and pass this into a PyTorch dataloader and separately create the model\n",
    "X = CR.cell_representation\n",
    "y = CR.labels\n",
    "\n",
    "print(f\"Cell representation X: {X.shape}\")\n",
    "print(f\"Cell labels y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Cell representation X: (26553, 1000)\n",
      "> Cell labels y: (26553,)\n",
      "> train_x.shape (21242, 1000)\n",
      "> validation_x.shape (2656, 1000)\n",
      "> test_x.shape (2655, 1000)\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "# PREPARE THE DATASET\n",
    "# I am including this explicit example here just for completeness, but this can\n",
    "# easily be rolled into a helper function\n",
    "########\n",
    "\n",
    "\n",
    "train_x, test_val_x, train_y, test_val_y = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "test_x, val_x, test_y, val_y = train_test_split(test_val_x, test_val_y, test_size=0.5, random_state=42) \n",
    "\n",
    "print(f\"> Cell representation X: {X.shape}\")\n",
    "print(f\"> Cell labels y: {y.shape}\")\n",
    "print(f\"> train_x.shape {train_x.shape}\")\n",
    "print(f\"> validation_x.shape {val_x.shape}\")\n",
    "print(f\"> test_x.shape {test_x.shape}\")\n",
    "\n",
    "# this is how you dynamically process your outputs into the right dataloader format\n",
    "# if you do not want conditional tokens, just omit those arguments\n",
    "# what is crucial is that the dataset contains the arguments `inputs` and `labels`, anything else will be put into `conditional`\n",
    "ds_train = Dataset.from_dict({\"inputs\": train_x,'labels':train_y, 'conditional_tokens_1': train_x, 'conditional_tokens_2': train_x})\n",
    "ds_valid= Dataset.from_dict({\"inputs\": val_x,'labels':val_y, 'conditional_tokens_1': val_x, 'conditional_tokens_2': val_x})\n",
    "ds_test = Dataset.from_dict({\"inputs\": test_x,'labels':test_y, 'conditional_tokens_1': test_x, 'conditional_tokens_2': test_x})\n",
    "\n",
    "## this can probably be rolled into the train functionality itself, but lets keep it outside to be eaiser to debug\n",
    "dataloader_train = DataLoader(ds_train, batch_size=int(config.dataset.task_args.batchsize), shuffle=config.dataset.task_args.shuffle, collate_fn=heimdall_collate_fn)\n",
    "dataloader_val = DataLoader(ds_valid, batch_size=int(config.dataset.task_args.batchsize), shuffle=config.dataset.task_args.shuffle, collate_fn=heimdall_collate_fn)\n",
    "dataloader_test = DataLoader(ds_test, batch_size=int(config.dataset.task_args.batchsize), shuffle=config.dataset.task_args.shuffle, collate_fn=heimdall_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': tensor([[647, 686, 365,  ...,  56, 759, 136],\n",
       "         [489, 811, 223,  ..., 554, 508, 498],\n",
       "         [919, 159,  60,  ..., 581, 645, 908],\n",
       "         ...,\n",
       "         [137, 181, 782,  ..., 554,  38, 498],\n",
       "         [211, 761, 745,  ..., 524, 872, 645],\n",
       "         [962, 251, 603,  ..., 640, 645, 554]]),\n",
       " 'labels': tensor([ 4, 11,  6, 16,  0,  4,  0,  0,  4,  2,  0,  0,  0,  0,  0,  0,  2, 16,\n",
       "          4,  6,  0,  3,  0,  6,  3,  4, 16, 16,  4, 11,  0,  0]),\n",
       " 'conditional_tokens': {'conditional_tokens_1': tensor([[647, 686, 365,  ...,  56, 759, 136],\n",
       "          [489, 811, 223,  ..., 554, 508, 498],\n",
       "          [919, 159,  60,  ..., 581, 645, 908],\n",
       "          ...,\n",
       "          [137, 181, 782,  ..., 554,  38, 498],\n",
       "          [211, 761, 745,  ..., 524, 872, 645],\n",
       "          [962, 251, 603,  ..., 640, 645, 554]]),\n",
       "  'conditional_tokens_2': tensor([[647, 686, 365,  ...,  56, 759, 136],\n",
       "          [489, 811, 223,  ..., 554, 508, 498],\n",
       "          [919, 159,  60,  ..., 581, 645, 908],\n",
       "          ...,\n",
       "          [137, 181, 782,  ..., 554,  38, 498],\n",
       "          [211, 761, 745,  ..., 524, 872, 645],\n",
       "          [962, 251, 603,  ..., 640, 645, 554]])}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in dataloader_train:\n",
    "    break\n",
    "\n",
    "## Demonstration of the dataset contents\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Instantiation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# Create the model and the types of inputs that it may use\n",
    "## `type` can either be `learned`, which is integer tokens and learned nn.embeddings, \n",
    "##  or `predefined`, which expects the dataset to prepare batchsize x length x hidden_dim\n",
    "#######\n",
    "\n",
    "conditional_input_types = {\n",
    "    \"conditional_tokens_1\":{\n",
    "        \"type\": \"learned\",\n",
    "        \"vocab_size\": 1000\n",
    "    },\n",
    "    \"conditional_tokens_2\":{\n",
    "        \"type\": \"learned\",\n",
    "        \"vocab_size\": 1000\n",
    "    }\n",
    "}\n",
    "\n",
    "## initialize the model\n",
    "from Heimdall.models import Heimdall_Transformer, TransformerConfig\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "## model config based on your specifications\n",
    "transformer_config = TransformerConfig(vocab_size = 1000, max_seq_length = 1000, prediction_dim = 20)\n",
    "model = Heimdall_Transformer(config=transformer_config, input_type=\"learned\", conditional_input_types = conditional_input_types)\n",
    "\n",
    "## optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.optimizer.learning_rate,\n",
    "    weight_decay=config.optimizer.weight_decay,\n",
    "    betas=(config.optimizer.beta1, config.optimizer.beta2),\n",
    "    foreach=False) ## the forearch is due to a distributed bug with cosine scheduler\n",
    "\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Using Device: cuda\n",
      "==> Starting a new WANDB run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnih121\u001b[0m (\u001b[33mHeimdall\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1aa48f6d2544d984c120dac2c3a4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0111126235117101, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/magroup/nzh/Heimdall/wandb/run-20240627_050558-7lxvsx4c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/Heimdall/Cell_Type_Classification/runs/7lxvsx4c' target=\"_blank\">run_name</a></strong> to <a href='https://wandb.ai/Heimdall/Cell_Type_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/Heimdall/Cell_Type_Classification' target=\"_blank\">https://wandb.ai/Heimdall/Cell_Type_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/Heimdall/Cell_Type_Classification/runs/7lxvsx4c' target=\"_blank\">https://wandb.ai/Heimdall/Cell_Type_Classification/runs/7lxvsx4c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Initialized Run\n",
      " !!! Remember that config batchsize here is GLOBAL Batchsize !!!\n",
      "> global batchsize: 32\n",
      "> num_devices: 1\n",
      "> total_samples: 21242\n",
      "> warmup_step: 663\n",
      "> total_steps: 6630\n",
      "> per_device_batch_size: 32\n",
      "> Finished Wrapping the model, optimizer, and dataloaders in accelerate\n",
      "> run Heimdall_Trainer.train() to begin training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<train.Heimdall_Trainer at 0x7f102fa6c250>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Heimdall_Trainer(config=config, model=model, optimizer=optimizer,\n",
    "                dataloader_train = dataloader_train, \n",
    "                dataloader_val = dataloader_val,\n",
    "                dataloader_test = dataloader_test,\n",
    "                run_wandb = True)\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:05<00:00, 14.83it/s]\n",
      "100%|██████████| 83/83 [00:04<00:00, 18.86it/s]\n",
      "Epoch: 0, Step 664, Loss: 0.7718, LR: 2.0e-03: 100%|██████████| 664/664 [01:00<00:00, 10.91it/s]\n",
      "100%|██████████| 83/83 [00:04<00:00, 20.61it/s]\n",
      "100%|██████████| 83/83 [00:04<00:00, 19.17it/s]\n",
      "Epoch: 1, Step 1328, Loss: 0.1148, LR: 1.9e-03: 100%|██████████| 664/664 [01:03<00:00, 10.48it/s]\n",
      "100%|██████████| 83/83 [00:04<00:00, 20.67it/s]\n",
      "100%|██████████| 83/83 [00:04<00:00, 19.73it/s]\n",
      "Epoch: 2, Step 1992, Loss: 0.0108, LR: 1.8e-03: 100%|██████████| 664/664 [01:01<00:00, 10.84it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 22.41it/s]\n",
      "100%|██████████| 83/83 [00:04<00:00, 19.18it/s]\n",
      "Epoch: 3, Step 2656, Loss: 0.0007, LR: 1.5e-03: 100%|██████████| 664/664 [01:02<00:00, 10.65it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 24.69it/s]\n",
      "100%|██████████| 83/83 [00:04<00:00, 17.88it/s]\n",
      "Epoch: 4, Step 3320, Loss: 0.0535, LR: 1.2e-03: 100%|██████████| 664/664 [01:02<00:00, 10.57it/s]\n",
      "100%|██████████| 83/83 [00:04<00:00, 20.21it/s]\n",
      "100%|██████████| 83/83 [00:04<00:00, 19.58it/s]\n",
      "Epoch: 5, Step 3984, Loss: 0.0465, LR: 8.2e-04: 100%|██████████| 664/664 [01:04<00:00, 10.30it/s]\n",
      "100%|██████████| 83/83 [00:04<00:00, 20.73it/s]\n",
      "100%|██████████| 83/83 [00:04<00:00, 18.48it/s]\n",
      "Epoch: 6, Step 4648, Loss: 0.0004, LR: 5.0e-04: 100%|██████████| 664/664 [01:05<00:00, 10.19it/s]\n",
      "100%|██████████| 83/83 [00:04<00:00, 20.33it/s]\n",
      "100%|██████████| 83/83 [00:04<00:00, 17.94it/s]\n",
      "Epoch: 7, Step 5312, Loss: 0.0006, LR: 2.3e-04: 100%|██████████| 664/664 [01:01<00:00, 10.77it/s]\n",
      "100%|██████████| 83/83 [00:04<00:00, 20.00it/s]\n",
      "100%|██████████| 83/83 [00:04<00:00, 17.63it/s]\n",
      "Epoch: 8, Step 5976, Loss: 0.0004, LR: 5.9e-05: 100%|██████████| 664/664 [01:00<00:00, 11.03it/s]\n",
      "100%|██████████| 83/83 [00:04<00:00, 19.29it/s]\n",
      "100%|██████████| 83/83 [00:04<00:00, 20.08it/s]\n",
      "Epoch: 9, Step 6640, Loss: 0.0264, LR: 1.1e-08: 100%|██████████| 664/664 [00:59<00:00, 11.08it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run_wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/magroup/nzh/Heimdall/train.py:139\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_wandb' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('heimdall')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c0ccaaad12502660836a31b85b28c12dfa2cc4f11816d6a33af9ba873ff4696"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
