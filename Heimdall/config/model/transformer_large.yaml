type: Heimdall.models.Transformer
name: transformer

args:
  d_model: 512
  pos_enc: BERT
  num_encoder_layers: 25
  nhead: 16
  hidden_act: gelu
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  use_flash_attn: false
  pooling: cls_pooling # or "mean_pooling"
