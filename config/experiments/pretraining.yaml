# @package _global_

###
# Set the experiment parts you would like to import
####
defaults:
  - override /model: transformer # the chosen model
  - override /dataset: new_sctab # dataset for the relevant task
  - override /tasks: new_sctab_split # dataset for the relevant task
  - override /scheduler: cosine # contains scheduler
  - override /optimizer: AdamW
  - override /fc: scbert
  - override /fe: binning_scbert
  - override /fg: esm2 #gene2vec
  - override /loss: CrossEntropyLoss

####
# Set the specific parameters you would like to change from the defaults
####

seed: 55

# run_name: ${fg.type}_${fe.type}_${fc.name}_${model.type}_lr${optimizer.args.lr}_bz${tasks.args.batchsize}  ## what is set for WandB
run_name: 'test_pretraining'
# work_dir: '${project_name}_results/${fg.type}_${fe.name}_${fc.type}_${dataset.dataset_name}_lr${optimizer.args.lr}_bz${tasks.args.batchsize}_seed${seed}_ag${fe.args.drop_zeros}'  ## set your custom working directory

project_name: 'pretraining' #project name for WandB

optimizer:
  args:
    lr: 2e-3


tasks:
  args:
    task_type: multiclass
    label_col_name: self_supervised
    metrics: null
    track_metric: null
    splits:
      type: predefined
      col: split3
      keys_:
        train: train
        val: val
        test: test
    shuffle: True
    batchsize: 64
    epochs: 5

    dataset_config:
      type: Heimdall.datasets.SeqMaskedPretrainDataset
    head_config:
      type: Heimdall.models.LinearSeqPredHead
      args: null
