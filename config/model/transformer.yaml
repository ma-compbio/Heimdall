type: Heimdall.models.HeimdallTransformer
name: transformer
args:
  d_model: 128
  pos_enc: BERT
  pooling: cls_pooling # or "mean_pooling"
  encoder_layer_parameters:
    type: torch.nn.TransformerEncoderLayer
    args:
      d_model: ${model.args.d_model}
      nhead: 2
      activation: gelu
      dropout: 0.1
      dim_feedforward: 512 # 4 * d_model
      batch_first: True
      norm_first: True
  encoder_parameters:
    type: torch.nn.TransformerEncoder
    args:
      num_layers: 2
