type: 'transformer'
args:
  hidden_size: 256
  num_hidden_layers: 6
  num_attention_heads: 8
  hidden_act: "gelu"
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 1024
  use_flash_attn: False
  pooling: "cls_pooling" # or "mean_pooling"